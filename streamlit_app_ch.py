# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

st.set_page_config(
    page_title="Abu Dhabi Real Estate â€” Peer Analysis",
    page_icon=":chart_with_upwards_trend:",
    layout="wide",
)
"""
# :material/query_stats: Abu Dhabi Real Estate Peer Analysis

Easily compare Project against others in their peer group.
"""

# =========================
# 1) è³‡æ–™è®€å–èˆ‡åŸºç¤æ¸…ç†
# =========================
@st.cache_data(show_spinner=False)
def load_data(file_path: str):
    df = pd.read_excel(file_path)
    orig_len = len(df)
    print(f"ğŸ“Š åŸå§‹è³‡æ–™ï¼š{orig_len:,} ç­†\n")
    
    # åŸºæœ¬æ–‡å­—æ­£è¦åŒ–
    for c in ["Project", "Community", "District"]:
        if c in df.columns:
            df[c] = df[c].astype(str).str.strip()
    
    # æ—¥æœŸ
    df["Registration"] = pd.to_datetime(df["Registration"], format="%m/%d/%y", errors="coerce")
    
    # æ•¸å€¼æ¬„ä½
    num_cols = ["Sold Area / GFA (sqm)", "Plot Area (sqm)", "Rate (AED/sqm)", "Price (AED)", "Share"]
    for c in num_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    
    # ä¾¿åˆ©æ¬„ä½
    df["YearMonth"] = df["Registration"].dt.to_period("M").dt.to_timestamp()
    
    # --- éæ¿¾ç„¡æ•ˆåˆ— ---
    # ç§»é™¤æ²’æ—¥æœŸ
    before_len = len(df)
    df = df[df["Registration"].notna()].copy()
    deleted = before_len - len(df)
    if deleted > 0:
        print(f"âŒ æ­¥é©Ÿ 1ï¼šç§»é™¤æ²’æ—¥æœŸçš„è¨˜éŒ„")
        print(f"   åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†\n")

    # é‡è¦ï¼šå…ˆåˆªé™¤ Price æˆ– Sold Area ä»»ä¸€ç‚ºç©ºçš„è¨˜éŒ„ï¼ˆæ‰èƒ½è¨ˆç®— Rateï¼‰
    if ("Price (AED)" in df.columns) and ("Sold Area / GFA (sqm)" in df.columns):
        before_len = len(df)
        df = df[df["Price (AED)"].notna() & df["Sold Area / GFA (sqm)"].notna()]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ æ­¥é©Ÿ 2ï¼šç§»é™¤ Price æˆ– Sold Area ä»»ä¸€ç‚ºç©ºçš„è¨˜éŒ„")
            print(f"   åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†\n")
    
    # Share æ¬„ä½éæ¿¾ï¼šåªä¿ç•™ Share = 1 (100%)
    if "Share" in df.columns:
        before_len = len(df)
        df = df[df["Share"] == 1]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ æ­¥é©Ÿ 3ï¼šåªä¿ç•™ Share = 1 çš„è¨˜éŒ„")
            print(f"   åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†\n")
        
    # ç§»é™¤éæ­£æ•¸ï¼ˆ<=0ï¼‰çš„é—œéµæ•¸å€¼
    for c in ["Sold Area / GFA (sqm)", "Plot Area (sqm)", "Rate (AED/sqm)", "Price (AED)"]:
        if c in df.columns:
            before_len = len(df)
            df = df[(df[c].isna()) | (df[c] > 0)]
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"âŒ æ­¥é©Ÿ 4ï¼šç§»é™¤éæ­£æ•¸çš„ {c}")
                print(f"   åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†\n")
    
    # ç§»é™¤ Project å±¬æ–¼ Private çš„åˆ—
    if "Project" in df.columns:
        before_len = len(df)
        df = df[~df["Project"].astype(str).str.contains("Private", case=False, na=False)]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ æ­¥é©Ÿ 5ï¼šç§»é™¤ Project å±¬æ–¼ Private çš„åˆ—")
            print(f"   åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†\n")
    
    # --- æ–°å¢ï¼šè‡ªå‹•è¨ˆç®— Rate (AED/sqm) ---
    # ç¾åœ¨ Price å’Œ Sold Area éƒ½ä¿è­‰æœ‰å€¼ï¼Œå¯ä»¥å®‰å¿ƒè¨ˆç®—
    if "Price (AED)" in df.columns and "Sold Area / GFA (sqm)" in df.columns:
        df["Rate_Calculated"] = df["Price (AED)"] / df["Sold Area / GFA (sqm)"]
        
        # æ–°å¢é©—è­‰æ¬„ä½ï¼šæ¯”è¼ƒåŸå§‹ Rate å’Œè¨ˆç®— Rate
        if "Rate (AED/sqm)" in df.columns:
            # è¨ˆç®—å·®ç•°ç™¾åˆ†æ¯”
            df["Rate_Match"] = np.where(
                (df["Rate (AED/sqm)"].notna()) & (df["Rate_Calculated"].notna()),
                np.abs(df["Rate (AED/sqm)"] - df["Rate_Calculated"]) / df["Rate_Calculated"] < 0.01,  # å…è¨± 1% èª¤å·®
                False  # åªè¦å…¶ä¸­ä¸€å€‹æ˜¯ç©ºæˆ–éƒ½æ˜¯ç©ºï¼Œéƒ½è¦–ç‚ºä¸åŒ¹é…
            )
            df["Rate_Difference"] = df["Rate (AED/sqm)"] - df["Rate_Calculated"]
            
            # è¼¸å‡ºä¸åŒ¹é…çš„è¨˜éŒ„çµ±è¨ˆ
            mismatch_count = (~df["Rate_Match"]).sum()
            if mismatch_count > 0:
                print(f"âš ï¸  è­¦å‘Šï¼šRate é©—è­‰")
                print(f"   ç™¼ç¾ {mismatch_count:,} ç­† Rate ä¸åŒ¹é…çš„è¨˜éŒ„")
                print(f"   ä¸åŒ¹é…ç‡ï¼š{mismatch_count/len(df)*100:.2f}%\n")
        else:
            # å¦‚æœåŸæœ¬æ²’æœ‰ Rate æ¬„ä½ï¼Œç”¨è¨ˆç®—çµæœå¡«å…¥
            df["Rate (AED/sqm)"] = df["Rate_Calculated"]
    
    # --- å»æ¥µå€¼ / éŒ¯èª¤å€¼ï¼ˆIQR æ³•ï¼‰---
    def _remove_outliers_iqr(df_in: pd.DataFrame, group_col: str, value_col: str, k: float = 3.0) -> pd.DataFrame:
        # åƒ…å°æ•¸å€¼å­˜åœ¨çš„åˆ—è¨ˆç®— IQRï¼ŒNaN ä¿ç•™
        sub = df_in[[group_col, value_col]].dropna()
        if sub.empty:
            return df_in
        q = sub.groupby(group_col)[value_col].quantile([0.25, 0.75]).unstack(level=-1)
        q.columns = ["q1", "q3"]
        q["iqr"] = q["q3"] - q["q1"]
        bounds = q.assign(
            lower=lambda x: x["q1"] - k * x["iqr"],
            upper=lambda x: x["q3"] + k * x["iqr"]
        )
        df_out = df_in.merge(bounds[["lower", "upper"]], left_on=group_col, right_index=True, how="left")
        mask_valid = (
            df_out[value_col].isna() |
            ((df_out[value_col] >= df_out["lower"]) & (df_out[value_col] <= df_out["upper"]))
        )
        df_out = df_out[mask_valid].drop(columns=["lower", "upper"])
        return df_out

    # é¸æ“‡å¯ç”¨çš„ç¾¤çµ„æ¬„ä½ï¼šProject > Community > District
    _group_col = None
    for _c in ["Project", "Community", "District"]:
        if _c in df.columns:
            _group_col = _c
            break

    if _group_col:
        print(f"ğŸ” æ­¥é©Ÿ 6ï¼šå»æ¥µå€¼åˆ†æï¼ˆä½¿ç”¨ {_group_col} åˆ†çµ„ï¼Œk=3.0ï¼‰")
        
        if "Rate (AED/sqm)" in df.columns:
            before_len = len(df)
            # Rate çš„æ¥µå€¼æª¢æ¸¬
            # k=3.0 æ˜¯æ¯”è¼ƒå¯¬é¬†çš„è¨­å®šï¼Œåªæœƒåˆªé™¤éå¸¸æ˜é¡¯çš„ç•°å¸¸å€¼
            df = _remove_outliers_iqr(df, _group_col, "Rate (AED/sqm)", k=3.0)
            # åˆªé™¤æ¯å€‹ Project ä¸­ Rate ç•°å¸¸é«˜æˆ–ç•°å¸¸ä½çš„è¨˜éŒ„
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"   âŒ ç§»é™¤ Rate (AED/sqm) æ¥µå€¼")
                print(f"      åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†")
        
        if "Price (AED)" in df.columns:
            before_len = len(df)
            # Price çš„æ¥µå€¼æª¢æ¸¬
            df = _remove_outliers_iqr(df, _group_col, "Price (AED)", k=3.0)
            # åˆªé™¤æ¯å€‹ Project ä¸­ Price ç•°å¸¸é«˜æˆ–ç•°å¸¸ä½çš„è¨˜éŒ„
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"   âŒ ç§»é™¤ Price (AED) æ¥µå€¼")
                print(f"      åˆªé™¤ï¼š{deleted:,} ç­† | å‰©é¤˜ï¼š{len(df):,} ç­†")
        
        print()
    
    cleaned_len = len(df)
    total_deleted = orig_len - cleaned_len
    print("="*50)
    print(f"âœ… è³‡æ–™æ¸…ç†å®Œæˆ")
    print(f"   åŸå§‹ï¼š{orig_len:,} ç­†")
    print(f"   åˆªé™¤ï¼š{total_deleted:,} ç­†ï¼ˆ{total_deleted/orig_len*100:.2f}%ï¼‰")
    print(f"   ä¿ç•™ï¼š{cleaned_len:,} ç­†ï¼ˆ{cleaned_len/orig_len*100:.2f}%ï¼‰")
    print("="*50)
    print()
    
    # æ¸…ç†å¾Œè¼¸å‡ºé©—è­‰çµæœ
    if "Rate_Match" in df.columns:
        match_count = df["Rate_Match"].sum()
        match_rate = match_count / len(df) * 100 if len(df) > 0 else 0
        print(f"âœ“ Rate é©—è­‰çµæœï¼š{match_count:,} ç­†è¨˜éŒ„åŒ¹é…ï¼ˆèª¤å·® < 1%ï¼ŒåŒ¹é…ç‡ {match_rate:.2f}%ï¼‰\n")
    
    # ä¿å­˜è³‡æ–™ï¼ˆç¢ºä¿æœ‰é©—è­‰æ¬„ä½ç”¨æ–¼å¾ŒçºŒæª¢æŸ¥ï¼‰
    df.to_csv("data/processed_data.csv", index=False, encoding='utf-8-sig')
    return df, {"orig_len": orig_len, "deleted": total_deleted, "cleaned_len": cleaned_len}

# é è¨­è·¯å¾‘å¯æ”¹
DATA_PATH = "data/data.xlsx"

try:
    df, stats = load_data(DATA_PATH)
except Exception as e:
    st.error(f"è®€å–æª”æ¡ˆå¤±æ•—ï¼š{e}")
    st.stop()

if df.empty:
    st.warning("è³‡æ–™ç‚ºç©ºï¼Œè«‹ç¢ºèª Excel æª”å…§å®¹ã€‚")
    st.stop()

# =========================
# 2) é é¢ä½ˆå±€ï¼šå·¦å´æ§åˆ¶ï¼Œå³å´çµæœ
# =========================
left, right = st.columns([1, 3])

# å·¦å´æ§åˆ¶æ¬„ä½ï¼ˆè³‡æ–™æ¦‚è¦½èˆ‡åƒæ•¸è¨­å®šï¼‰
with left:
    st.markdown("### è³‡æ–™ä¾†æºæ¦‚è¦½")
    st.caption(f"ç­†æ•¸ï¼š{len(df):,}ï¼ŒæœŸé–“ï¼š{df['Registration'].min().date()} â†’ {df['Registration'].max().date()}")

    group_dim = st.selectbox(
        "åŒå„•ç¾¤çµ„ç¶­åº¦",
        options=["Community", "Project", "District"],
        index=1
    )

    metric = st.selectbox(
        "åˆ†ææŒ‡æ¨™",
        options=["Rate (AED/sqm)", "Price (AED)"],
        index=0,
        help="é è¨­ä½¿ç”¨æ¯å¹³æ–¹å…¬å°ºå–®åƒ¹ï¼›ä¹Ÿå¯åˆ‡æ›ç‚ºäº¤æ˜“ç¸½åƒ¹åšå°æ¯”ã€‚"
    )

    agg_fn_name = st.selectbox(
        "èšåˆæ–¹å¼",
        options=["median", "mean"],
        index=0,
        help="æ¯æœˆå°ç¾¤çµ„å…§å¤šç­†äº¤æ˜“åšèšåˆï¼ˆå¸¸ç”¨ median æŠ—é›¢ç¾¤å€¼ï¼‰ã€‚"
    )

    freq = st.selectbox(
        "æ™‚é–“é »ç‡",
        options=["Monthly", "Quarterly"],
        index=0
    )

    horizon_label = st.pills(
        "æ™‚é–“è¦–çª—",
        options=["3M", "6M", "1Y", "3Y", "5Y", "Max"],
        default="1Y",
    )

# =========================
# 3) æ™‚é–“éæ¿¾èˆ‡é »ç‡è½‰æ›
# =========================
def to_freq(df: pd.DataFrame, freq: str) -> pd.DataFrame:
    # å…ˆä¾ç¾¤çµ„+æœˆä»½èšåˆï¼Œå†åšé »ç‡ä¸Šå·ï¼ˆM / Qï¼‰
    base = (
        df.dropna(subset=["Registration", group_dim, metric])
          .groupby([group_dim, "YearMonth"])[metric]
    )

    if agg_fn_name == "median":
        g = base.median().reset_index()
    elif agg_fn_name == "mean":
        g = base.mean().reset_index()
    elif agg_fn_name == "count":
        g = base.count().reset_index().rename(columns={metric: f"{metric} Count"})
    else:
        g = base.median().reset_index()

    # è½‰é »ç‡
    if freq == "Monthly":
        g = g.rename(columns={"YearMonth": "Date"})
    else:  # Quarterly
        g["Date"] = g["YearMonth"].dt.to_period("Q").dt.to_timestamp(how="end")
        g = g.groupby([group_dim, "Date"]).agg({metric: "median" if agg_fn_name != "count" else "sum"}).reset_index()

    return g

agg_ts = to_freq(df, freq)

# æ™‚é–“è¦–çª—è¨ˆç®—
end_date = agg_ts["Date"].max()
if horizon_label == "3M":
    start_date = end_date - pd.DateOffset(months=3)
elif horizon_label == "6M":
    start_date = end_date - pd.DateOffset(months=6)
elif horizon_label == "1Y":
    start_date = end_date - pd.DateOffset(years=1)
elif horizon_label == "3Y":
    start_date = end_date - pd.DateOffset(years=3)
elif horizon_label == "5Y":
    start_date = end_date - pd.DateOffset(years=5)
else:
    start_date = agg_ts["Date"].min()

agg_ts = agg_ts[(agg_ts["Date"] >= start_date) & (agg_ts["Date"] <= end_date)]

# =========================
# 4) å³å´é ‚éƒ¨ï¼šé¸æ“‡ç¾¤çµ„èˆ‡æŒ‡æ¨™é¡¯ç¤º
# =========================
with right:
    st.markdown(f"### é¸æ“‡è¦å°æ¯”çš„ {group_dim}")

    # ä¾æœŸé–“å…§ç¾¤çµ„åç¨±å­—æ¯æ’åºï¼Œä¾¿æ–¼æŒ‘é¸
    sub_df = df[(df["Registration"] >= start_date) & (df["Registration"] <= end_date)]
    options = (
        sub_df[group_dim]
          .dropna()
          .astype(str)
          .unique()
          .tolist()
    )
    # Alphabetical sort (case-insensitive)
    options = sorted(options, key=lambda s: s.lower())
    #import re

    # é—œéµå­—æ¨¡å¼ï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰
    # é—œéµå­—åˆ—è¡¨ï¼ˆä¸åˆ†å¤§å°å¯«åŒ¹é…ï¼‰
    default_pick = [
        "Park View Residence, Al Saadiyat Island",
        "Saadiyat Grove - The Source Residences",
        "Saadiyat Grove - The Source Terraces",
        "Saadiyat Grove - The Source",
        "Saadiyat Grove - The Arthouse",
        "Louvre Residences",
        "Canal Residence",
        "SAAS Heights",
        "Mayan",
    ]

    # æ ¹æ“šæ­£å‰‡è‡ªå‹•åŒ¹é… options ä¸­çš„åç¨±
            # default_pick = [
            #     name for name in options
            #     if any(re.search(p, name, re.IGNORECASE) for p in patterns)
            # ]

    picked_groups = st.multiselect(
        f"é¸æ“‡è¦å°æ¯”çš„ {group_dim}",
        options=options,
        default=default_pick,
        placeholder=f"è¼¸å…¥æˆ–é¸æ“‡ {group_dim} åç¨±"
    )

    if not picked_groups:
        st.info("è«‹è‡³å°‘é¸ä¸€å€‹ç¾¤çµ„ã€‚")
        st.stop()

# =========================
# 5) å³å´é ‚éƒ¨ï¼šæœŸé–“ç›¸å°è¡¨ç¾ï¼ˆèµ·é»=1ï¼‰æŒ‡æ¨™èˆ‡èªªæ˜æ–‡å­—
# =========================
with right:
    # è½‰å¯¬è¡¨ã€æ­£è¦åŒ–ï¼ˆèµ·é»=1ï¼‰ã€åŒå„•å¹³å‡
    pivot = agg_ts.pivot(index="Date", columns=group_dim, values=(metric if agg_fn_name != "count" else f"{metric} Count"))
    pivot = pivot.sort_index()

    # åƒ…ä¿ç•™ä½¿ç”¨è€…æŒ‘çš„ç¾¤çµ„
    missing = [g for g in picked_groups if g not in pivot.columns]
    picked_groups = [g for g in picked_groups if g in pivot.columns]

    if len(picked_groups) == 0:
        st.error("é¸æ“‡çš„ç¾¤çµ„åœ¨ç›®å‰æ™‚é–“çª—å…§æ²’æœ‰è³‡æ–™ã€‚")
        st.stop()
    if missing:
        st.warning(f"ä»¥ä¸‹ç¾¤çµ„åœ¨ç›®å‰æ™‚é–“çª—å…§ç„¡è³‡æ–™ï¼Œå·²å¿½ç•¥ï¼š{', '.join(missing)}")

    sub = pivot[picked_groups].dropna(how="all")
    # å»æ‰å…¨æ˜¯ NaN çš„åˆ—
    sub = sub.dropna(axis=0, how="all")

    # æ­£è¦åŒ–ï¼ˆå„ç¾¤çµ„åœ¨æœŸé–“ç¬¬ä¸€å€‹éç©ºå€¼ç‚º 1ï¼‰
    def normalize_df(df_wide: pd.DataFrame) -> pd.DataFrame:
        norm = df_wide.copy()
        for c in norm.columns:
            series = norm[c].dropna()
            if series.empty:
                norm[c] = np.nan
            else:
                first = series.iloc[0]
                norm[c] = norm[c] / first
        return norm

    normalized = normalize_df(sub)

    latest_vals = normalized.iloc[-1].dropna()
    if not latest_vals.empty:
        best_name = latest_vals.idxmax()
        best_val = latest_vals.max()
        worst_name = latest_vals.idxmin()
        worst_val = latest_vals.min()

        st.markdown("### æœŸé–“ç›¸å°è¡¨ç¾ï¼ˆèµ·é»=1ï¼‰")
        c1, c2 = st.columns(2)
        c1.metric("æœ€ä½³ç¾¤çµ„", best_name, delta=f"{round((best_val - 1) * 100)}%")
        c2.metric("æœ€å¼±ç¾¤çµ„", worst_name, delta=f"{round((worst_val - 1) * 100)}%")

    st.caption("""
    - æŒ‡æ¨™è§£è®€ï¼šæ­£è¦åŒ–=1 è¡¨ç¤ºæœŸé–“èµ·é»ï¼›æœ€å¾Œå€¼ 1.25 â‰ˆ æœŸé–“ç´¯è¨ˆ +25%ã€‚
    - å»ºè­°å„ªå…ˆç”¨ `Rate (AED/sqm)` + `median`ï¼Œå¯æ¸›å°‘è±ªå®…æ¥µå€¼å°å¹³å‡çš„å¹²æ“¾ã€‚
    - æƒ³åšæ›´åš´è¬¹ã€ŒåŒå„•é›†ã€ï¼šå¯æ”¹ç‚ºåŒå€ `District` æˆ–åŒç”¢å“å‹åˆ¥ `Property Type` çš„å­é›†åˆã€‚
    """)

# =========================
# 6) å³å´ä¸­æ®µï¼šç¸½è¦½åœ–ï¼ˆæ­£è¦åŒ–æŠ˜ç·šï¼‰
# =========================
with right:
    st.markdown("## æ­£è¦åŒ–èµ°å‹¢ï¼ˆèµ·é»=1ï¼‰")
    chart_df = normalized.reset_index().melt(id_vars="Date", var_name=group_dim, value_name="Normalized")
    fig = px.line(
        chart_df, x="Date", y="Normalized", color=group_dim,
        height=420,
        hover_data={group_dim: True, "Normalized": ":.3f", "Date": "|%Y-%m-%d"},
    )
    fig.update_yaxes(title=None)
    fig.update_xaxes(title=None)
    st.plotly_chart(fig, width='stretch')

# =========================
# 7) å³å´åº•éƒ¨ï¼šå€‹åˆ¥ vs åŒå„•å¹³å‡ + Deltaï¼ˆminus peer averageï¼‰
# =========================
with right:
    if len(picked_groups) >= 2:
        st.markdown("## å€‹åˆ¥ç¾¤çµ„ vs åŒå„•å¹³å‡")
        grid_cols = st.columns(4)

        for i, gname in enumerate(picked_groups):
            peers = normalized.drop(columns=[gname])
            peer_avg = peers.mean(axis=1)

            # (a) è©²ç¾¤çµ„ vs åŒå„•å¹³å‡
            comp_df = pd.DataFrame({
                "Date": normalized.index,
                gname: normalized[gname],
                "Peer average": peer_avg
            })
            comp_df = comp_df.melt(id_vars="Date", var_name="Series", value_name="Value")

            fig1 = px.line(
                comp_df, x="Date", y="Value", color="Series",
                height=300,
                title=f"{gname} vs Peer average",
                hover_data={"Value": ":.3f", "Date": "|%Y-%m-%d"},
                color_discrete_map={gname: "red", "Peer average": "gray"} 
            )
            fig1.update_yaxes(title=None, rangemode="tozero")
            fig1.update_xaxes(title=None)
            grid_cols[(i * 2) % 4].plotly_chart(fig1, width='stretch')

            # (b) Deltaï¼šè©²ç¾¤çµ„ - åŒå„•å¹³å‡
            delta_df = pd.DataFrame({
                "Date": normalized.index,
                "Delta": normalized[gname] - peer_avg
            }).dropna()

            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(
                x=delta_df["Date"], y=delta_df["Delta"],
                mode="lines", fill="tozeroy", name="Delta"
            ))
            fig2.update_layout(
                title=f"{gname} minus Peer average",
                height=300, showlegend=False, margin=dict(l=10, r=10, t=40, b=10)
            )
            fig2.update_yaxes(zeroline=True, zerolinewidth=1)
            grid_cols[(i * 2 + 1) % 4].plotly_chart(fig2, width='stretch')
    else:
        st.info("è¦çœ‹ vs åŒå„•å¹³å‡èˆ‡ Deltaï¼Œè«‹è‡³å°‘é¸ 2 å€‹ç¾¤çµ„ã€‚")

# =========================
# 8) åŸå§‹/èšåˆè³‡æ–™
# =========================
# with st.expander("æŸ¥çœ‹èšåˆå¾Œçš„æ™‚åºè³‡æ–™", expanded=False):
#     st.dataframe(pivot, width='stretch')
with st.expander("æŸ¥çœ‹åŸå§‹è³‡æ–™ï¼ˆç¶“æ¸…æ´—ï¼‰", expanded=False):
    # æ¸…ç†çµ±è¨ˆé¡¯ç¤º
    if isinstance(stats, dict) and all(k in stats for k in ("orig_len", "deleted", "cleaned_len")) and stats["orig_len"]:
        st.code(
            f"""
               åŸå§‹ï¼š{stats['orig_len']:,} ç­†
               åˆªé™¤ï¼š{stats['deleted']:,} ç­†ï¼ˆ{stats['deleted']/stats['orig_len']*100:.2f}%ï¼‰
               ä¿ç•™ï¼š{stats['cleaned_len']:,} ç­†ï¼ˆ{stats['cleaned_len']/stats['orig_len']*100:.2f}%ï¼‰
            """,
            language="text",
        )
    raw = df[(df["Registration"] >= start_date) & (df["Registration"] <= end_date)].copy()
    st.dataframe(raw, width='stretch')