# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

st.set_page_config(
    page_title="Abu Dhabi Real Estate â€” Peer Analysis",
    page_icon=":chart_with_upwards_trend:",
    layout="wide",
)
"""
# :material/query_stats: Abu Dhabi Real Estate Peer Analysis

Easily compare Project against others in their peer group.
"""

# =========================
# 1) è³‡æ–™è®€å–èˆ‡åŸºç¤æ¸…ç†
# =========================
@st.cache_data(show_spinner=False)
def load_data(file_path: str):
    df = pd.read_excel(file_path)
    orig_len = len(df)
    print(f"ğŸ“Š Raw rows: {orig_len:,}\n")
    
    # åŸºæœ¬æ–‡å­—æ­£è¦åŒ–
    for c in ["Project", "Community", "District"]:
        if c in df.columns:
            df[c] = df[c].astype(str).str.strip()
    
    # æ—¥æœŸ
    df["Registration"] = pd.to_datetime(df["Registration"], format="%m/%d/%y", errors="coerce")
    
    # æ•¸å€¼æ¬„ä½
    num_cols = ["Sold Area / GFA (sqm)", "Plot Area (sqm)", "Rate (AED/sqm)", "Price (AED)", "Share"]
    for c in num_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    
    # ä¾¿åˆ©æ¬„ä½
    df["YearMonth"] = df["Registration"].dt.to_period("M").dt.to_timestamp()
    
    # --- éæ¿¾ç„¡æ•ˆåˆ— ---
    # ç§»é™¤æ²’æ—¥æœŸ
    before_len = len(df)
    df = df[df["Registration"].notna()].copy()
    deleted = before_len - len(df)
    if deleted > 0:
        print(f"âŒ Step 1: Removed records without Registration date")
        print(f"   Deleted: {deleted:,} | Remaining: {len(df):,}\n")

    # é‡è¦ï¼šå…ˆåˆªé™¤ Price æˆ– Sold Area ä»»ä¸€ç‚ºç©ºçš„è¨˜éŒ„ï¼ˆæ‰èƒ½è¨ˆç®— Rateï¼‰
    if ("Price (AED)" in df.columns) and ("Sold Area / GFA (sqm)" in df.columns):
        before_len = len(df)
        df = df[df["Price (AED)"].notna() & df["Sold Area / GFA (sqm)"].notna()]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ Step 2: Remove records with missing Price or Sold Area")
            print(f"   Deleted: {deleted:,} | Remaining: {len(df):,}\n")
    
    # Share æ¬„ä½éæ¿¾ï¼šåªä¿ç•™ Share = 1 (100%)
    if "Share" in df.columns:
        before_len = len(df)
        df = df[df["Share"] == 1]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ Step 3: Keep only Share = 1 records")
            print(f"   Deleted: {deleted:,} | Remaining: {len(df):,}\n")
        
    # ç§»é™¤éæ­£æ•¸ï¼ˆ<=0ï¼‰çš„é—œéµæ•¸å€¼
    for c in ["Sold Area / GFA (sqm)", "Plot Area (sqm)", "Rate (AED/sqm)", "Price (AED)"]:
        if c in df.columns:
            before_len = len(df)
            df = df[(df[c].isna()) | (df[c] > 0)]
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"âŒ Step 4: Remove non-positive values in {c}")
                print(f"   Deleted: {deleted:,} | Remaining: {len(df):,}\n")
    
    # ç§»é™¤ Project å±¬æ–¼ Private çš„åˆ—
    if "Project" in df.columns:
        before_len = len(df)
        df = df[~df["Project"].astype(str).str.contains("Private", case=False, na=False)]
        deleted = before_len - len(df)
        if deleted > 0:
            print(f"âŒ Step 5: Remove rows where Project is Private")
            print(f"   Deleted: {deleted:,} | Remaining: {len(df):,}\n")
    
    # --- æ–°å¢ï¼šè‡ªå‹•è¨ˆç®— Rate (AED/sqm) ---
    # ç¾åœ¨ Price å’Œ Sold Area éƒ½ä¿è­‰æœ‰å€¼ï¼Œå¯ä»¥å®‰å¿ƒè¨ˆç®—
    if "Price (AED)" in df.columns and "Sold Area / GFA (sqm)" in df.columns:
        df["Rate_Calculated"] = df["Price (AED)"] / df["Sold Area / GFA (sqm)"]

        # æ–°å¢é©—è­‰æ¬„ä½ï¼šæ¯”è¼ƒåŸå§‹ Rate å’Œè¨ˆç®— Rate
        if "Rate (AED/sqm)" in df.columns:
            # è¨ˆç®—å·®ç•°ç™¾åˆ†æ¯”
            df["Rate_Match"] = np.where(
                (df["Rate (AED/sqm)"].notna()) & (df["Rate_Calculated"].notna()),
                np.abs(df["Rate (AED/sqm)"] - df["Rate_Calculated"]) / df["Rate_Calculated"] < 0.01,  # å…è¨± 1% èª¤å·®
                False  # åªè¦å…¶ä¸­ä¸€å€‹æ˜¯ç©ºæˆ–éƒ½æ˜¯ç©ºï¼Œéƒ½è¦–ç‚ºä¸åŒ¹é…
            )
            df["Rate_Difference"] = df["Rate (AED/sqm)"] - df["Rate_Calculated"]

            # è¼¸å‡ºä¸åŒ¹é…çš„è¨˜éŒ„çµ±è¨ˆ
            mismatch_count = (~df["Rate_Match"]).sum()
            if mismatch_count > 0:
                print(f"âš ï¸  Warning: Rate validation")
                print(f"   Found {mismatch_count:,} records with mismatched Rate")
                print(f"   Mismatch rate: {mismatch_count/len(df)*100:.2f}%\n")
        else:
            # å¦‚æœåŸæœ¬æ²’æœ‰ Rate æ¬„ä½ï¼Œç”¨è¨ˆç®—çµæœå¡«å…¥
            df["Rate (AED/sqm)"] = df["Rate_Calculated"]
    
    # --- å»æ¥µå€¼ / éŒ¯èª¤å€¼ï¼ˆIQR æ³•ï¼‰---
    def _remove_outliers_iqr(df_in: pd.DataFrame, group_col: str, value_col: str, k: float = 3.0) -> pd.DataFrame:
        # åƒ…å°æ•¸å€¼å­˜åœ¨çš„åˆ—è¨ˆç®— IQRï¼ŒNaN ä¿ç•™
        sub = df_in[[group_col, value_col]].dropna()
        if sub.empty:
            return df_in
        q = sub.groupby(group_col)[value_col].quantile([0.25, 0.75]).unstack(level=-1)
        q.columns = ["q1", "q3"]
        q["iqr"] = q["q3"] - q["q1"]
        bounds = q.assign(
            lower=lambda x: x["q1"] - k * x["iqr"],
            upper=lambda x: x["q3"] + k * x["iqr"]
        )
        df_out = df_in.merge(bounds[["lower", "upper"]], left_on=group_col, right_index=True, how="left")
        mask_valid = (
            df_out[value_col].isna() |
            ((df_out[value_col] >= df_out["lower"]) & (df_out[value_col] <= df_out["upper"]))
        )
        df_out = df_out[mask_valid].drop(columns=["lower", "upper"])
        return df_out

    # é¸æ“‡å¯ç”¨çš„ç¾¤çµ„æ¬„ä½ï¼šProject > Community > District
    _group_col = None
    for _c in ["Project", "Community", "District"]:
        if _c in df.columns:
            _group_col = _c
            break

    if _group_col:
        print(f"ğŸ” Step 6: Outlier removal (grouped by {_group_col}, k=3.0)")

        if "Rate (AED/sqm)" in df.columns:
            before_len = len(df)
            # Rate çš„æ¥µå€¼æª¢æ¸¬
            # k=3.0 æ˜¯æ¯”è¼ƒå¯¬é¬†çš„è¨­å®šï¼Œåªæœƒåˆªé™¤éå¸¸æ˜é¡¯çš„ç•°å¸¸å€¼
            df = _remove_outliers_iqr(df, _group_col, "Rate (AED/sqm)", k=3.0)
            # åˆªé™¤æ¯å€‹ Project ä¸­ Rate ç•°å¸¸é«˜æˆ–ç•°å¸¸ä½çš„è¨˜éŒ„
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"   âŒ Remove outliers in Rate (AED/sqm)")
                print(f"      Deleted: {deleted:,} | Remaining: {len(df):,}")

        if "Price (AED)" in df.columns:
            before_len = len(df)
            # Price çš„æ¥µå€¼æª¢æ¸¬
            df = _remove_outliers_iqr(df, _group_col, "Price (AED)", k=3.0)
            # åˆªé™¤æ¯å€‹ Project ä¸­ Price ç•°å¸¸é«˜æˆ–ç•°å¸¸ä½çš„è¨˜éŒ„
            deleted = before_len - len(df)
            if deleted > 0:
                print(f"   âŒ Remove outliers in Price (AED)")
                print(f"      Deleted: {deleted:,} | Remaining: {len(df):,}")

        print()
    
    cleaned_len = len(df)
    total_deleted = orig_len - cleaned_len
    print("="*50)
    print(f"âœ… Data cleaning completed")
    print(f"   Original: {orig_len:,} rows")
    print(f"   Deleted: {total_deleted:,} ({total_deleted/orig_len*100:.2f}%)")
    print(f"   Kept: {cleaned_len:,} ({cleaned_len/orig_len*100:.2f}%)")
    print("="*50)
    print()
    
    # æ¸…ç†å¾Œè¼¸å‡ºé©—è­‰çµæœ
    if "Rate_Match" in df.columns:
        match_count = df["Rate_Match"].sum()
        match_rate = match_count / len(df) * 100 if len(df) > 0 else 0
        print(f"âœ“ Rate validation: {match_count:,} records matched (error < 1%, match rate {match_rate:.2f}%)\n")
    
    # ä¿å­˜è³‡æ–™ï¼ˆç¢ºä¿æœ‰é©—è­‰æ¬„ä½ç”¨æ–¼å¾ŒçºŒæª¢æŸ¥ï¼‰
    df.to_csv("data/processed_data.csv", index=False, encoding='utf-8-sig')
    return df, {"orig_len": orig_len, "deleted": total_deleted, "cleaned_len": cleaned_len}

# é è¨­è·¯å¾‘å¯æ”¹
DATA_PATH = "data/data.xlsx"

try:
    df, stats = load_data(DATA_PATH)
except Exception as e:
    st.error(f"Failed to read file: {e}")
    st.stop()

if df.empty:
    st.warning("Dataframe is empty. Please check the Excel file.")
    st.stop()

# =========================
# 2) é é¢ä½ˆå±€ï¼šå·¦å´æ§åˆ¶ï¼Œå³å´çµæœ
# =========================
left, right = st.columns([1, 3])

# å·¦å´æ§åˆ¶æ¬„ä½ï¼ˆè³‡æ–™æ¦‚è¦½èˆ‡åƒæ•¸è¨­å®šï¼‰
with left:
    st.markdown("### Data overview")
    st.caption(f"Rows: {len(df):,}, Period: {df['Registration'].min().date()} â†’ {df['Registration'].max().date()}, Update frequency: Biweekly")

    group_dim = st.selectbox(
        "Peer group dimension",
        options=["Community", "Project", "District"],
        index=1
    )

    metric_display = st.selectbox(
        "Metric",
        options=["Rate (AED/sqft)", "Rate (AED/sqm)"],
        index=0,
        help="Default: price per sqft; switch to sqm for comparison."
    )

    # æ˜ å°„åˆ°å¯¦éš›æ•¸æ“šæ¬„ä½ (åº•å±¤éƒ½ä½¿ç”¨ sqm)
    metric = "Rate (AED/sqm)"
    # è¨˜éŒ„æ˜¯å¦éœ€è¦å–®ä½è½‰æ›
    convert_to_sqft = (metric_display == "Rate (AED/sqft)")

    agg_fn_name = st.selectbox(
        "Aggregation",
        options=["median", "mean"],
        index=0,
        help="Aggregate monthly transactions per group (median is more robust to outliers)."
    )

    freq = st.selectbox(
        "Time frequency",
        options=["Monthly", "Quarterly"],
        index=0
    )

    horizon_label = st.pills(
        "Time window",
        options=["1M", "3M", "6M", "1Y", "3Y", "5Y", "Max"],
        default="1Y",
    )

    # è‡ªå®šç¾©æ—¥æœŸé¸æ“‡å™¨ (å§‹çµ‚é¡¯ç¤º)
    #st.markdown("#### Custom date range")
    col1, col2 = st.columns(2)
    with col1:
        custom_start = st.date_input("Start date", value=None)
    with col2:
        custom_end = st.date_input("End date", value=None)

# =========================
# 3) æ™‚é–“éæ¿¾èˆ‡é »ç‡è½‰æ›
# =========================
def to_freq(df: pd.DataFrame, freq: str) -> pd.DataFrame:
    # å…ˆä¾ç¾¤çµ„+æœˆä»½èšåˆï¼Œå†åšé »ç‡ä¸Šå·ï¼ˆM / Qï¼‰
    base = (
        df.dropna(subset=["Registration", group_dim, metric])
          .groupby([group_dim, "YearMonth"])[metric]
    )

    if agg_fn_name == "median":
        g = base.median().reset_index()
    elif agg_fn_name == "mean":
        g = base.mean().reset_index()
    elif agg_fn_name == "count":
        g = base.count().reset_index().rename(columns={metric: f"{metric} Count"})
    else:
        g = base.median().reset_index()

    # è½‰é »ç‡
    if freq == "Monthly":
        g = g.rename(columns={"YearMonth": "Date"})
    else:  # Quarterly
        g["Date"] = g["YearMonth"].dt.to_period("Q").dt.to_timestamp(how="end")
        g = g.groupby([group_dim, "Date"]).agg({metric: "median" if agg_fn_name != "count" else "sum"}).reset_index()

    return g

agg_ts = to_freq(df, freq)

# æ™‚é–“è¦–çª—è¨ˆç®—
end_date = agg_ts["Date"].max()

# å„ªå…ˆä½¿ç”¨è‡ªå®šç¾©æ—¥æœŸ(å¦‚æœå…©å€‹æ—¥æœŸéƒ½æœ‰é¸æ“‡)
if custom_start is not None and custom_end is not None:
    start_date = pd.Timestamp(custom_start)
    end_date = pd.Timestamp(custom_end)
# å¦å‰‡ä½¿ç”¨ pills é¸æ“‡çš„æ™‚é–“ç¯„åœ
elif horizon_label == "1M":
    start_date = end_date - pd.DateOffset(months=1)
elif horizon_label == "3M":
    start_date = end_date - pd.DateOffset(months=3)
elif horizon_label == "6M":
    start_date = end_date - pd.DateOffset(months=6)
elif horizon_label == "1Y":
    start_date = end_date - pd.DateOffset(years=1)
elif horizon_label == "3Y":
    start_date = end_date - pd.DateOffset(years=3)
elif horizon_label == "5Y":
    start_date = end_date - pd.DateOffset(years=5)
else:  # Max
    start_date = agg_ts["Date"].min()

agg_ts = agg_ts[(agg_ts["Date"] >= start_date) & (agg_ts["Date"] <= end_date)]

# å¦‚æœç”¨æˆ¶é¸æ“‡ sqft,è½‰æ›å–®ä½ (1 sqm = 10.764 sqft)
if convert_to_sqft:
    agg_ts[metric] = agg_ts[metric] / 10.764

# =========================
# 4) å³å´é ‚éƒ¨ï¼šé¸æ“‡ç¾¤çµ„èˆ‡æŒ‡æ¨™é¡¯ç¤º
# =========================
with right:
    st.markdown(f"### Select {group_dim} to compare")

    # ä¾æœŸé–“å…§ç¾¤çµ„åç¨±å­—æ¯æ’åºï¼Œä¾¿æ–¼æŒ‘é¸
    sub_df = df[(df["Registration"] >= start_date) & (df["Registration"] <= end_date)]
    options = (
        sub_df[group_dim]
          .dropna()
          .astype(str)
          .unique()
          .tolist()
    )
    # Alphabetical sort (case-insensitive)
    options = sorted(options, key=lambda s: s.lower())
    #import re

    # é—œéµå­—æ¨¡å¼ï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰
    # é—œéµå­—åˆ—è¡¨ï¼ˆä¸åˆ†å¤§å°å¯«åŒ¹é…ï¼‰
    default_pick_candidates = [
        "Park View Residence, Al Saadiyat Island",
        "Saadiyat Grove - The Source Residences",
        "Saadiyat Grove - The Source Terraces",
        "Saadiyat Grove - The Source",
        "Saadiyat Grove - The Arthouse",
        "Louvre Residences",
        "Canal Residence",
        "SAAS Heights",
        "Mayan",
    ]

    # æ ¹æ“šæ­£å‰‡è‡ªå‹•åŒ¹é… options ä¸­çš„åç¨±
            # default_pick = [
            #     name for name in options
            #     if any(re.search(p, name, re.IGNORECASE) for p in patterns)
            # ]

    # åªä¿ç•™åœ¨ç•¶å‰æ™‚é–“ç¯„åœå…§å­˜åœ¨çš„é è¨­é¸é …
    default_pick = [item for item in default_pick_candidates if item in options]

    picked_groups = st.multiselect(
        f"Pick {group_dim} to compare",
        options=options,
        default=default_pick,
        placeholder=f"Type or select a {group_dim} name"
    )

    if not picked_groups:
        st.info("Please pick at least one group.")
        st.stop()

# =========================
# 5) å³å´é ‚éƒ¨ï¼šæœŸé–“ç›¸å°è¡¨ç¾ï¼ˆèµ·é»=1ï¼‰æŒ‡æ¨™èˆ‡èªªæ˜æ–‡å­—
# =========================
with right:
    # è½‰å¯¬è¡¨ã€æ­£è¦åŒ–ï¼ˆèµ·é»=1ï¼‰ã€åŒå„•å¹³å‡
    pivot = agg_ts.pivot(index="Date", columns=group_dim, values=(metric if agg_fn_name != "count" else f"{metric} Count"))
    pivot = pivot.sort_index()

    # åƒ…ä¿ç•™ä½¿ç”¨è€…æŒ‘çš„ç¾¤çµ„
    missing = [g for g in picked_groups if g not in pivot.columns]
    picked_groups = [g for g in picked_groups if g in pivot.columns]

    if len(picked_groups) == 0:
        st.error("The selected groups have no data in the current window.")
        st.stop()
    if missing:
        st.warning(f"No data for the following groups in the current window; ignored: {', '.join(missing)}")

    sub = pivot[picked_groups].dropna(how="all")
    # å»æ‰å…¨æ˜¯ NaN çš„åˆ—
    sub = sub.dropna(axis=0, how="all")

    # æ­£è¦åŒ–ï¼ˆå„ç¾¤çµ„åœ¨æœŸé–“ç¬¬ä¸€å€‹éç©ºå€¼ç‚º 1ï¼‰
    def normalize_df(df_wide: pd.DataFrame) -> pd.DataFrame:
        norm = df_wide.copy()
        for c in norm.columns:
            series = norm[c].dropna()
            if series.empty:
                norm[c] = np.nan
            else:
                first = series.iloc[0]
                norm[c] = norm[c] / first
        return norm

    normalized = normalize_df(sub)

    latest_vals = normalized.iloc[-1].dropna()
    if not latest_vals.empty:
        best_name = latest_vals.idxmax()
        best_val = latest_vals.max()
        worst_name = latest_vals.idxmin()
        worst_val = latest_vals.min()

        st.markdown("### Relative performance over window (base=1)")
        c1, c2 = st.columns(2)
        c1.metric("Top group", best_name, delta=f"{round((best_val - 1) * 100)}%")
        c2.metric("Weakest group", worst_name, delta=f"{round((worst_val - 1) * 100)}%")

    st.caption("""
    - How to read: normalization=1 is window start; final value 1.25 â‰ˆ +25% over the window.
    - Tip: prefer `Rate (AED/sqft)` or `Rate (AED/sqm)` with `median` to reduce luxury outlier skew.
    - For a stricter peer set: filter by the same `District` or `Property Type`.
    """)

# =========================
# 6) å³å´ä¸­æ®µï¼šç¸½è¦½åœ–ï¼ˆæ­£è¦åŒ–æŠ˜ç·šï¼‰
# =========================
with right:
    st.markdown("## Normalized trend (base=1)")
    chart_df = normalized.reset_index().melt(id_vars="Date", var_name=group_dim, value_name="Normalized")
    fig = px.line(
        chart_df, x="Date", y="Normalized", color=group_dim,
        height=420,
        hover_data={group_dim: True, "Normalized": ":.3f", "Date": "|%Y-%m-%d"},
    )
    fig.update_yaxes(title=None)
    fig.update_xaxes(title=None)
    st.plotly_chart(fig, use_container_width=True)

# =========================
# 7) å³å´åº•éƒ¨ï¼šå€‹åˆ¥ vs åŒå„•å¹³å‡ + Deltaï¼ˆminus peer averageï¼‰
# =========================
with right:
    if len(picked_groups) >= 2:
        st.markdown("## Each group vs peer average")
        grid_cols = st.columns(4)

        for i, gname in enumerate(picked_groups):
            peers = normalized.drop(columns=[gname])
            peer_avg = peers.mean(axis=1)

            # (a) è©²ç¾¤çµ„ vs åŒå„•å¹³å‡
            comp_df = pd.DataFrame({
                "Date": normalized.index,
                gname: normalized[gname],
                "Peer average": peer_avg
            })
            comp_df = comp_df.melt(id_vars="Date", var_name="Series", value_name="Value")

            fig1 = px.line(
                comp_df, x="Date", y="Value", color="Series",
                height=300,
                title=f"{gname} vs Peer average",
                hover_data={"Value": ":.3f", "Date": "|%Y-%m-%d"},
                color_discrete_map={gname: "red", "Peer average": "gray"} 
            )
            fig1.update_yaxes(title=None, rangemode="tozero")
            fig1.update_xaxes(title=None)
            grid_cols[(i * 2) % 4].plotly_chart(fig1, use_container_width=True)

            # (b) Deltaï¼šè©²ç¾¤çµ„ - åŒå„•å¹³å‡
            delta_df = pd.DataFrame({
                "Date": normalized.index,
                "Delta": normalized[gname] - peer_avg
            }).dropna()

            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(
                x=delta_df["Date"], y=delta_df["Delta"],
                mode="lines", fill="tozeroy", name="Delta"
            ))
            fig2.update_layout(
                title=f"{gname} minus Peer average",
                height=300, showlegend=False, margin=dict(l=10, r=10, t=40, b=10)
            )
            fig2.update_yaxes(zeroline=True, zerolinewidth=1)
            grid_cols[(i * 2 + 1) % 4].plotly_chart(fig2, use_container_width=True)
    else:
        st.info("Select at least 2 groups to view vs peer average and delta.")

# =========================
# 8) åŸå§‹/èšåˆè³‡æ–™
# =========================
# with st.expander("æŸ¥çœ‹èšåˆå¾Œçš„æ™‚åºè³‡æ–™", expanded=False):
#     st.dataframe(pivot, use_container_width=True)
with st.expander("View raw data (cleaned)", expanded=False):
    # æ¸…ç†çµ±è¨ˆé¡¯ç¤º
    if isinstance(stats, dict) and all(k in stats for k in ("orig_len", "deleted", "cleaned_len")) and stats["orig_len"]:
        st.code(
            f"""
               Original: {stats['orig_len']:,} rows
               Deleted: {stats['deleted']:,} ({stats['deleted']/stats['orig_len']*100:.2f}%)
               Kept: {stats['cleaned_len']:,} ({stats['cleaned_len']/stats['orig_len']*100:.2f}%)
            """,
            language="text",
        )
    raw = df[(df["Registration"] >= start_date) & (df["Registration"] <= end_date)].copy()
    st.dataframe(raw, use_container_width=True)